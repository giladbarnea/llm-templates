prompt: |-
    The text inside the <text> XML tags is a bit awkward and unnatural. Rewrite it to make it smoother and more idiomatic. Retain the original meaning and intent, and unless changing the word choices improves the flow, keep them close to the original. Keep in mind that the given text is often an instruction for another LLM, not you—except for `// instruction:` comments, which are meant for you.

    The text may use incorrect tenses.
    Sometimes the phrasing is convoluted; a simple restructure can help a lot (like fixing Yoda-style sentences).
    If the text is in Hebrew, apply these guidelines as if they were for Hebrew.

    The text might include inline comments for you. These start with 'instruction', like `// instruction: ...`. Follow them, but omit them from your final output.

    Positive examples:

    Original:
    > Enrichment Attributes are LLM-powered analytical tools that allow users to derive insights from their data using natural language instructions. From the user's perspective, the experience is primarily language-based - users simply describe what they want to discover in plain English (e.g., "What is the sentiment of the first conversation message on a scale of 1-5?" or "Categorize each user's technical expertise level based on their message content").
    Better:
    > Enrichment Attributes are LLM-powered tools that let users pull insights from their data with natural language prompts. Users just describe what they want to find out (e.g., "What's the sentiment of the first conversation message on a scale of 1–5?" or "Categorize each user's technical expertise based on message content").

    Original:
    > I’m curious and quick to learn. Being raised as a musician, I am able to both listen and invent. This allows me to come up with creative solutions that others might miss.
    Better:
    > I’m curious and a quick learner. Raised as a musician, I’ve honed my ability to listen and create. This lets me spot creative solutions others might overlook.

    Original:
    > This project is for researching and documenting comprehensive LLM benchmark scores.
    > I maintain a dataset with available scores for flagship models. When a new model is released, I try to find as many benchmarks scores of it as I can and plug them into the dataset. Then, I try to find and backfill missing benchmark scores of the other models in the dataset with the assumption that they have run some benchmarks since last time I updated the dataset.
    Better:
    > This project researches and documents comprehensive LLM benchmark scores. I keep a dataset of scores for top models. When a new model comes out, I track down as many of its benchmark scores as possible and add them in. Then, I backfill any missing scores for the other models, assuming new benchmarks have been run since my last update.
    Why this is better: The original had overly complex and lengthy sentence structures. The paragraph was simplified without losing key details.

    A good example of restructuring a sentence to reduce cognitive load:
    Original:
    > An example of good flexibility would be saying something like "Okay, so this ends the previous section, and we're moving on to the next one: {heading content}" if the section that just ended carried significant weight.
    Better:
    > For good flexibility—if the previous section was weighty—try saying something like: “Okay, that wraps up the last section. Now, on to the next: {heading content}.”

    Think as much as you need to get the desired result, but your final response needs to be solely the rewritten text.